{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo = 2500\n",
    "data_pandas = pd.read_csv('./data/games_cleaned_'+str(elo)+'_scrambled.csv')\n",
    "# print(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(labels : list[str]):\n",
    "    labels_dict = {'.': 0, 'K': 1, 'Q': 2, 'B': 3, 'N': 4, 'R': 5, 'P': 6, 'k': 7, 'q': 8, 'b': 9, 'n': 10, 'r': 11, 'p': 12}\n",
    "    encoded_labels = torch.zeros(len(labels), len(labels_dict))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i][labels_dict[label]] = 1\n",
    "    return encoded_labels.view(-1, 64*13) # will return a 1x832 which is 64x13\n",
    "\n",
    "def encode_player_col(player : str):\n",
    "    player_col = torch.Tensor([1 if player == 'white' else 0])\n",
    "    return player_col\n",
    "\n",
    "class Neuro_gambit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neuro_gambit, self).__init__()\n",
    "        # self.lin_test = nn.Linear(833, 36) # input 64*13 + 1, output 36\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(833, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 36)\n",
    "        )\n",
    "\n",
    "    def forward_pandas(self, positions : list[str], player : str):\n",
    "        pos_tensor = one_hot_encode_labels(positions)\n",
    "        player_col = encode_player_col(player)\n",
    "        input_layer = torch.cat((pos_tensor, player_col.unsqueeze(1)), dim=1)\n",
    "        output = self.decoder(self.encoder(input_layer))\n",
    "        return torch.split(output, [8, 8, 8, 8, 4], dim=1)\n",
    "\n",
    "    def forward(self, x : torch.Tensor):\n",
    "        output = self.decoder(self.encoder(x))\n",
    "        return torch.split(output, [8, 8, 8, 8, 4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: white ['.', '.', 'r', '.', '.', 'r', 'k', '.', '.', '.', '.', '.', '.', 'p', 'b', 'p', 'p', '.', 'n', '.', '.', '.', 'p', '.', '.', '.', '.', 'N', 'p', '.', '.', '.', '.', '.', '.', '.', 'P', '.', '.', '.', '.', '.', '.', '.', 'B', '.', '.', '.', 'P', 'P', 'P', '.', '.', 'P', 'P', 'P', 'R', '.', '.', '.', '.', 'R', 'K', '.']\n",
      "output: (tensor([[ 0.0354,  0.0462, -0.0158, -0.0344,  0.0426, -0.0260, -0.0312,  0.0661]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1150,  0.0716, -0.0906,  0.0854, -0.0111, -0.0385,  0.0046,  0.0915]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1050, -0.0398,  0.0477,  0.0266, -0.0222, -0.1043,  0.0748, -0.0829]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[ 0.0136,  0.0031, -0.0075, -0.0248,  0.0590, -0.0426, -0.0302,  0.0771]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.0580, -0.0923, -0.1206,  0.0049]],\n",
      "       grad_fn=<SplitWithSizesBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model = Neuro_gambit()\n",
    "position_columns = ['p'+str(i) for i in range(64)]\n",
    "test_position = data_pandas[position_columns].iloc[0].to_list()\n",
    "test_player = data_pandas['player'].iloc[0]\n",
    "\n",
    "print('input:', test_player,test_position)\n",
    "\n",
    "model = Neuro_gambit()\n",
    "forward_test = model.forward_pandas(test_position, test_player)\n",
    "\n",
    "print('output:', forward_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_uci(uci : str):\n",
    "    pos_rank_labels = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
    "    pos_file_labels = {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7}\n",
    "    promotion_labels = {'q': 0, 'r': 1, 'b': 2, 'n': 3}\n",
    "\n",
    "    origin_pos = uci[:2]\n",
    "    destin_pos = uci[2:4]\n",
    "\n",
    "    org_rank = origin_pos[0]\n",
    "    org_file = origin_pos[1]\n",
    "\n",
    "    des_rank = destin_pos[0]\n",
    "    des_file = destin_pos[1]\n",
    "\n",
    "    # org pos\n",
    "    encoded_org_pos_rank = torch.zeros(1, len(pos_rank_labels))\n",
    "    encoded_org_pos_rank[0][pos_rank_labels[org_rank]] = 1\n",
    "\n",
    "    encoded_org_pos_file = torch.zeros(1, len(pos_file_labels))\n",
    "    encoded_org_pos_file[0][pos_file_labels[org_file]] = 1\n",
    "\n",
    "    # des pos\n",
    "    encoded_des_pos_rank = torch.zeros(1, len(pos_rank_labels))\n",
    "    encoded_des_pos_rank[0][pos_rank_labels[des_rank]] = 1\n",
    "\n",
    "    encoded_des_pos_file = torch.zeros(1, len(pos_file_labels))\n",
    "    encoded_des_pos_file[0][pos_file_labels[des_file]] = 1\n",
    "\n",
    "    encoded_prom = torch.zeros(1, len(promotion_labels)) # all zeros\n",
    "\n",
    "    if (len(uci) == 5): # there is a promotion\n",
    "        promotion = uci[4]\n",
    "        for i, label in enumerate(promotion):\n",
    "            encoded_prom[i][promotion_labels[label]] = 1\n",
    "\n",
    "    return torch.cat((encoded_org_pos_rank.view(1, -1), encoded_org_pos_file.view(1, -1),\n",
    "                      encoded_des_pos_rank.view(1, -1), encoded_des_pos_file.view(1, -1),\n",
    "                      encoded_prom.view(1, -1)), dim=1)\n",
    "\n",
    "def decode_uci(tensors : list[torch.Tensor]):\n",
    "    pos_rank_labels = {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h'}\n",
    "    pos_file_labels = {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8'}\n",
    "    promotion_labels = {0: 'q', 1: 'r', 2: 'b', 3: 'n'}\n",
    "\n",
    "    tensor_list = []\n",
    "    for i in range(len(tensors)):\n",
    "        tensor_list.append(tensors[i].tolist()[0])\n",
    "    o_r = pos_rank_labels[tensor_list[0].index(1)]\n",
    "    o_f = pos_file_labels[tensor_list[1].index(1)]\n",
    "    d_r = pos_rank_labels[tensor_list[2].index(1)]\n",
    "    d_f = pos_file_labels[tensor_list[3].index(1)]\n",
    "    try:\n",
    "        p = promotion_labels[tensor_list[4].index(1)]\n",
    "    except ValueError:\n",
    "        p = ''\n",
    "    return o_r+o_f+d_r+d_f+p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2c3\n",
      "c2c3\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "# test2 = data_pandas['uci'].iloc[418]\n",
    "test2 = data_pandas['uci'].iloc[0]\n",
    "print(test2)\n",
    "t = encode_uci(test2)\n",
    "t_split = torch.split(t, [8, 8, 8, 8, 4], dim=1)\n",
    "\n",
    "print(decode_uci(t_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1266, 833])\n",
      "torch.Size([1266, 36])\n"
     ]
    }
   ],
   "source": [
    "# Transforming dataframe to tensor\n",
    "position_columns = ['p'+str(i) for i in range(64)]\n",
    "\n",
    "X_pos_pandas = data_pandas[position_columns]\n",
    "X_col_pandas = data_pandas['player']\n",
    "\n",
    "X_pos_values = X_pos_pandas.values\n",
    "X_col_values = X_col_pandas.values\n",
    "X_list = []\n",
    "i = 0\n",
    "for i in range(len(X_pos_values)):\n",
    "    x_pos = X_pos_values[i]\n",
    "    x_pos_encoded = one_hot_encode_labels(x_pos)\n",
    "    x_col = X_col_values[i]\n",
    "    x_col_encoded = encode_player_col(x_col)\n",
    "\n",
    "    x = torch.cat((x_pos_encoded, x_col_encoded.unsqueeze(1)), dim=1)\n",
    "    X_list.append(x)\n",
    "\n",
    "X = torch.cat(X_list, dim=0)\n",
    "print(X.shape)\n",
    "torch.save(X, './data/X_tensor_'+str(elo)+'.pt')\n",
    "\n",
    "\n",
    "Y_pandas = data_pandas['uci']\n",
    "Y_pandas_values = Y_pandas.values\n",
    "Y_list = []\n",
    "for i in range(len(Y_pandas_values)):\n",
    "    y_val = Y_pandas_values[i]\n",
    "    y = encode_uci(y_val)\n",
    "    Y_list.append(y)\n",
    "\n",
    "Y = torch.cat(Y_list, dim=0)\n",
    "print(Y.shape)\n",
    "torch.save(Y, './data/Y_tensor_'+str(elo)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # using cuda\n",
    "model = Neuro_gambit().to(device)\n",
    "\n",
    "# data\n",
    "X : torch.Tensor = torch.load('./data/X_tensor_'+str(elo)+'.pt').to(device)\n",
    "Y : torch.Tensor = torch.load('./data/Y_tensor_'+str(elo)+'.pt').to(device)\n",
    "\n",
    "# seperating the Y\n",
    "Y1 = Y[:, :8]\n",
    "Y2 = Y[:, 8:16]\n",
    "Y3 = Y[:, 16:24]\n",
    "Y4 = Y[:, 24:32]\n",
    "Y5 = Y[:, 32:]\n",
    "\n",
    "Y = [Y1,Y2,Y3,Y4,Y5]\n",
    "\n",
    "# epochs, loss, and optim\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000000\n",
    "\n",
    "# loss and optimizer functions from pytorch\n",
    "criterion = nn.MSELoss() # MSE function\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate) # stochastic gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1105/100000], Loss: 0.0712\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[308], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     y_preds \u001b[39m=\u001b[39m model(X) \u001b[39m# will output a tuple of 5 tensors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_preds)): \u001b[39m# calculating the loss per tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[298], line 40\u001b[0m, in \u001b[0;36mNeuro_gambit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x : torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     39\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x))\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msplit(output, [\u001b[39m8\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m4\u001b[39;49m], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py:189\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    184\u001b[0m         split, (tensor,), tensor, split_size_or_sections, dim\u001b[39m=\u001b[39mdim)\n\u001b[1;32m    185\u001b[0m \u001b[39m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m# call here.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msplit(split_size_or_sections, dim)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:803\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_VF\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m, split_size, dim)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_VF\u001b[39m.\u001b[39;49msplit_with_sizes(\u001b[39mself\u001b[39;49m, split_size, dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # forward\n",
    "    y_preds = model(X) # will output a tuple of 5 tensors\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(len(y_preds)): # calculating the loss per tensor\n",
    "        y_pred = y_preds[i]\n",
    "        total_loss += criterion(y_pred, Y[i])\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0: # every 100 steps\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss.item():.4f}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2c3\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0.1095, 0.1478, 0.2360, 0.1051, 0.0943, 0.1062, 0.0969, 0.1041],\n",
      "       device='cuda:0')\n",
      "tensor([0.1017, 0.2905, 0.0976, 0.0970, 0.1001, 0.1062, 0.1031, 0.1037],\n",
      "       device='cuda:0')\n",
      "tensor([0.0977, 0.1327, 0.2648, 0.1042, 0.1175, 0.0955, 0.0737, 0.1139],\n",
      "       device='cuda:0')\n",
      "tensor([0.1008, 0.1041, 0.2454, 0.1177, 0.1143, 0.1149, 0.0988, 0.1039],\n",
      "       device='cuda:0')\n",
      "tensor([0.2517, 0.2514, 0.2463, 0.2506], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# sample test\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(data_pandas.iloc[0]['uci'])\n",
    "    x = X[0].view(1,-1)\n",
    "    y = Y[0][0],Y[1][0],Y[2][0],Y[3][0],Y[4][0]\n",
    "    y_preds = [t.view(-1) for t in model(x)]\n",
    "    for y_iter in y:\n",
    "        print(y_iter)\n",
    "    for y_pred in y_preds:\n",
    "        print(F.softmax(y_pred, dim=0))\n",
    "    # print(criterion(y_pred,y).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), './models/neuro_gambit_'+str(elo)+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
